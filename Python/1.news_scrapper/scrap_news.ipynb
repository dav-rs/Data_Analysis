{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News extraction with `BeautifulSoup`\n",
    "This notebook is the first part of a project for training an LDA model. Here we will obtain a collection of news articles from Australian news. In order to get this corpus we will use BeautifulSoup library to scrap the news articles. In a second notebook we will proceed to **pre-process** the data and transform it to train an LDA model. To see the second part of the project please check the folder 'LDA_model_and_vis'.\n",
    "\n",
    "### Web Scrapper - getting the corpus\n",
    "For the first section we proceed to obtain some news articles from Australian news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:33:34.068766Z",
     "start_time": "2020-08-04T06:33:31.316774Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlopen # library to open URLs\n",
    "import requests # get the html content\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify URL with data and use requests to get html content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:38:26.861786Z",
     "start_time": "2020-08-04T06:38:26.363310Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/'\n",
    "r1 = requests.get(url)\n",
    "coverpage = r1.content #html content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:38:29.084961Z",
     "start_time": "2020-08-04T06:38:28.413042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(coverpage, 'html5lib') # breaks html file into python objects\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all tags corresponding to h3 titles and get the `title-container` class from them to access the content (for this specific website, in other html we may require another class). In this section we are just having an idea of the structure to get the contents systematically in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:59:34.545815Z",
     "start_time": "2020-08-04T06:59:34.540963Z"
    }
   },
   "outputs": [],
   "source": [
    "# soup # shows the source code of the website. Look for h3 tags and get the name of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:41:51.372176Z",
     "start_time": "2020-08-04T06:41:51.339273Z"
    }
   },
   "outputs": [],
   "source": [
    "coverpage_news = soup.find_all('h3',class_='fc-item__title') # use the name of the class for h3 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:44:21.460848Z",
     "start_time": "2020-08-04T06:44:21.453775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3 class=\"fc-item__title\"><a class=\"fc-item__link\" data-link-name=\"article\" href=\"https://www.theguardian.com/australia-news/2020/aug/04/covid-australia-police-officer-in-victoria-allegedly-brutally-bashed-by-anti-masker\"><span class=\"fc-item__kicker\">Melbourne</span> <span class=\"u-faux-block-link__cta fc-item__headline\"> <span class=\"js-headline-text\">Police officer allegedly brutally bashed by anti-masker</span></span> </a></h3>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T06:44:42.531578Z",
     "start_time": "2020-08-04T06:44:42.526337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Melbourne  Police officer allegedly brutally bashed by anti-masker '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[4].get_text() # check text of fifth object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T07:17:06.503119Z",
     "start_time": "2020-08-04T07:17:06.497457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.theguardian.com/australia-news/2020/aug/04/covid-australia-police-officer-in-victoria-allegedly-brutally-bashed-by-anti-masker'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[4].find('a')['href']\n",
    "# coverpage_news[4].find('a').get_text()\n",
    "# requests.get(str(coverpage_news[4].find('a')['href']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap the first 50 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T13:20:34.480740Z",
     "start_time": "2020-08-04T13:20:34.470447Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "# Function for the live_news section which contains a different structure with json format\n",
    "def live_news(soup_obj, tit_list, cont_list):\n",
    "    body = soup_obj.find_all('script', type='application/ld+json') # get script tag for live news section\n",
    "    for article in body: # for each of the found tags \n",
    "        json_file = json.loads(article.text) # get the text (a dictionary of the json format)\n",
    "\n",
    "        try:\n",
    "            json_txt = functools.reduce(operator.iconcat, json_file['liveBlogUpdate'], []) # flat list of dicts            \n",
    "            full_dict = {d['headline']:d['articleBody'] for d in json_txt} # dict of headline and article content\n",
    "            for key,value in full_dict.items():\n",
    "                tit_list.append(key)\n",
    "                cont_list.append(value)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T13:32:28.213140Z",
     "start_time": "2020-08-04T13:31:52.842885Z"
    }
   },
   "outputs": [],
   "source": [
    "num_articles = 50\n",
    "\n",
    "cont_list = []\n",
    "links_list = []\n",
    "titles_list = []\n",
    "\n",
    "for n in np.arange(0,num_articles): # for each article\n",
    "    link = coverpage_news[n].find('a')['href'] # get the link of news article\n",
    "    links_list.append(link)\n",
    "    \n",
    "    article = requests.get(str(link)) # request the link from the main url \n",
    "    cont = article.content # get the article html\n",
    "\n",
    "    soup_article = BeautifulSoup(cont,'html5lib') # BS object\n",
    "    body = soup_article.find_all('div', {'class':['content__article-body from-content-api js-article__body',\n",
    "                                                  'article-body-commercial-selector css-79elbk']}) # get the content in the specific class_\n",
    "    try:\n",
    "        x = body[0].find_all('p') # get only the p (paragraph) tags\n",
    "    except:\n",
    "        live_news(soup_article, titles_list, cont_list)\n",
    "\n",
    "    else:\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0,len(x)): # for each p tag in the body (contained in the variable x)\n",
    "            paragraph = x[p].get_text() # get the text of each paragraph\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = ' '.join(list_paragraphs) # put all paragraphs together\n",
    "\n",
    "        cont_list.append(final_article)\n",
    "        title = coverpage_news[n].find('a').get_text() # get the title of news article\n",
    "        titles_list.append(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the extraction process we obtained 68 articles (the live news sections contain more than one title, therefore we end up with more than 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T13:34:49.692384Z",
     "start_time": "2020-08-04T13:34:49.685369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"  \\n\\n\\n  Bolt's talent for speed becomes more apparent now it is denied to us \",\n",
       " 'Something you may not know about me: there is almost no set of circumstances – personal, professional, medical – in which I will not drop everything to watch Usain Bolt. Naturally, my personalised YouTube algorithm has already known this for some time, and will now instantly recommend me a selection')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(titles_list))\n",
    "print(len(cont_list))\n",
    "titles_list[50],cont_list[50][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "After exploring the html content of the selected news webpage we were able to extract the first 50 articles of the main page. During this process we came across with two types of articles (Live updates and regular article) which required two different approaches to be extracted. The first one uses a json format for the content and the second one was obtained with the use of the `div` tag with the corresponding `class` for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
